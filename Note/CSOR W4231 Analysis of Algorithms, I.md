# CSOR W4231 Analysis of Algorithms, I

<div align = "center"><font size = 5> Tong Wu, tw2906 <div>

[TOC]

## 0. Course Overview

### 0.1 Algorithms

An algorithm is a **well-defined** computational procedure that transforms the **input** (a set of values) into the **output** (a new set of values). 

The desired I/O relationship is specified by the statement of the **computational problem** for which the algorithm is designed.

An algorithm is **correct** if, *for every input*, it **halts** with the correct output.

> ç®—æ³•æ˜¯ä¸€ä¸ªè¢«æ˜ç¡®å®šä¹‰çš„è®¡ç®—è¿‡ç¨‹ã€‚å®ƒå°†ä¸€ç»„è¾“å…¥å€¼è½¬æ¢ä¸ºä¸€ç»„æ–°çš„è¾“å‡ºå€¼ã€‚
>
> æœŸæœ›çš„è¾“å…¥è¾“å‡ºå…³ç³»ç”±è®¾è®¡ç®—æ³•çš„è®¡ç®—é—®é¢˜æ‰€å£°æ˜æŒ‡å®šã€‚
>
> å¯¹äºæ¯ä¸ªè¾“å…¥ï¼Œéƒ½èƒ½ä»¥æ­£ç¡®çš„è¾“å‡ºä½œä¸ºåœæ­¢ï¼Œåˆ™ç®—æ³•æ˜¯æ­£ç¡®çš„ã€‚

### 0.2 Efficient Algorithms

Efficiency is related to the resources an algorithm uses: **time, space**

- How much time/space are used?
- How do they *scale* as the input size grows?

#### 0.2.1 Running time

Running time = number of **primitive computational steps** performed; typically these are

1. arithmetic operations: add, subtract, multiply, divide *fixed-size* integers
2. data movement operations: load, store, copy
3. control operations: branching, subroutine call and return

> è¿è¡Œæ—¶é—´=æ‰€æ‰§è¡Œçš„åŸå§‹è®¡ç®—æ­¥éª¤çš„æ•°é‡ï¼›é€šå¸¸è¿™äº›æ­¥éª¤æ˜¯:
>
> 1.ç®—æœ¯æ“ä½œï¼šåŠ ã€å‡ã€ä¹˜ã€é™¤å›ºå®šå¤§å°çš„æ•´æ•°
> 2.æ•°æ®ç§»åŠ¨æ“ä½œï¼šåŠ è½½ã€å­˜å‚¨ã€å¤åˆ¶
> 3.æ§åˆ¶æ“ä½œï¼šåˆ†æ”¯ã€å­ç¨‹åºè°ƒç”¨å’Œè¿”å›

## 1. Insertion Sort and Efficient Algorithms

### 1.1 Sorting problem

- Input: A list $A$ of $n$ integers $x_1,...,x_n$
- Output: A permutation (æ’åˆ—ç»„åˆ) $x_1',x_2',...,x_n'$ of the $n$ integers **where they are sorted in non-decreasing order**, i.e., $x_1'\le x_2'\le ... \le x_n'$

Example:

- Input: $n=6,\ A=\{9,3,2,6,8,5\}$
- Output: $A=\{2,3,5,6,8,9\}$

The *data structure* should use to represent the list of output is: 

**Array:** collection of items of the same data type

- allows for random access
- â€œzeroâ€ indexed in C++ and Java

### 1.2 Insertion sort

![image-20230209141858037](https://images.wu.engineer/images/2023/02/09/image-20230209141858037.png)

![img](https://www.runoob.com/wp-content/uploads/2019/03/insertionSort.gif)

1. Start with a (trivially) sorted subarray of size 1 consisting of $A[1]$
2. Increase the size of the sorted subarray by 1, by inserting the next element A, call it **key**, in the correct position in the **sorted** subarray to its left.
   - Compare key with every element $x$ in the sorted subarray to the left of key, starting from the right.
     - If $x>key$, move $x$ one position to the right
     - If $x\le key$, insert key after $x$
3. Repeat Step 2. until the sorted subarray has size $n$.

> 1. ä»ä¸€ä¸ªæœ‰åºåºåˆ—å¼€å§‹ï¼Œå³å°†ç¬¬ä¸€ä¸ªå…ƒç´ çœ‹ä½œä¸€ä¸ªæœ‰åºåºåˆ—(sorted subarray)ï¼Œç§°ä½œä¸ºæ•°ç»„A[1]ï¼Œå…¶ä½™çš„è¢«ç§°ä¸ºæœªæ’åºåºåˆ—(unsorted subarray)
> 2. å°†æ­¤æœ‰åºåºåˆ—çš„å¤§å°å¢åŠ 1ï¼Œæ–¹æ³•æ˜¯å°†ä¸‹ä¸€ä¸ªå…ƒç´ A(key)ï¼Œæ’å…¥åˆ°æœ‰åºæ•°ç»„çš„é€‚å½“ä½ç½®ã€‚
>    - å°†keyå’Œæ¯ä¸ªæœ‰åºåºåˆ—ä¸­çš„å…ƒç´ xè¿›è¡Œæ¯”è¾ƒï¼Œä»æœ€å³è¾¹çš„æ•°å­—å¼€å§‹ã€‚
>      - å¦‚æœ$x>key$ï¼Œå°†$x$å‘å³ç§»ä¸€æ ¼
>      - å¦‚æœ$x\le key$ï¼Œå°†keyæ’å…¥åœ¨xçš„åé¢
>
> 3. é‡å¤ç¬¬äºŒæ­¥ï¼Œç›´åˆ°æœªæ’åºåºåˆ—ä¸º0

#### 1.2.1 Example of insertion sort

$n=6,A=\{9,3,2,6,8,5\}$

![image-20230209143549253](https://images.wu.engineer/images/2023/02/09/image-20230209143549253.png)

#### 1.2.2 Code

**Pseudo Code**

Let $A$ be an array of $n$ integers

```pseudocode
insertion-sort(A)
	for i=2 to n do
		key = A[i]
		// Insert A[i] into the sorted subarray A[1,i-1]
		j = i - 1
		while j>0 and A[j]>key do
			A[j+1] = A[j]
			j = j - 1
		end while
		A[j+1] = key
	end for
```

**C++**

```c++
void insertion_sort(int arr[],int len){
	for(int i=1;i<len;i++){
		int key=arr[i];
		int j=i-1;
		while((j>=0) && (key<arr[j])){
			arr[j+1]=arr[j];
			j--;
		}
		arr[j+1]=key;
	}
}
```

**Python**

```python
def insertionSort(arr):
    for i in range(len(arr)):
        preIndex = i-1
        current = arr[i]
        while preIndex >= 0 and arr[preIndex] > current:
            arr[preIndex+1] = arr[preIndex]
            preIndex-=1
        arr[preIndex+1] = current
    return arr
```

**Java**

```java
public class InsertSort implements IArraySort {
    @Override
    public int[] sort(int[] sourceArray) throws Exception {
        // å¯¹ arr è¿›è¡Œæ‹·è´ï¼Œä¸æ”¹å˜å‚æ•°å†…å®¹
        int[] arr = Arrays.copyOf(sourceArray, sourceArray.length);
        // ä»ä¸‹æ ‡ä¸º1çš„å…ƒç´ å¼€å§‹é€‰æ‹©åˆé€‚çš„ä½ç½®æ’å…¥ï¼Œå› ä¸ºä¸‹æ ‡ä¸º0çš„åªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œé»˜è®¤æ˜¯æœ‰åºçš„
        for (int i = 1; i < arr.length; i++) {
            // è®°å½•è¦æ’å…¥çš„æ•°æ®
            int tmp = arr[i];
            // ä»å·²ç»æ’åºçš„åºåˆ—æœ€å³è¾¹çš„å¼€å§‹æ¯”è¾ƒï¼Œæ‰¾åˆ°æ¯”å…¶å°çš„æ•°
            int j = i;
            while (j > 0 && tmp < arr[j - 1]) {
                arr[j] = arr[j - 1];
                j--;
            }
            // å­˜åœ¨æ¯”å…¶å°çš„æ•°ï¼Œæ’å…¥
            if (j != i) {
                arr[j] = tmp;
            }
        }
        return arr;
    }
}
```

### 1.3 Analysis of algorithms

- Correctness: formal proof often by induction

- Running time: number of primitive computational steps
  - Not the same as time it takes to execute the algorithm
  - We want to measure that is independent of hardware
  - We want to know how running time scales with the size of the input
- Space: how much space is required by the algorithm

> æ­£ç¡®æ€§ï¼šé€šå¸¸é€šè¿‡å½’çº³æ³•è¿›è¡Œæ­£å¼è¯æ˜
>
> è¿è¡Œæ—¶é—´ï¼šåŸå§‹è®¡ç®—æ­¥éª¤çš„æ•°é‡
>
> â€‹	- ä¸æ‰§è¡Œç®—æ³•çš„æ—¶é—´ä¸ä¸€æ ·
>
> â€‹	- æˆ‘ä»¬å¸Œæœ›æµ‹é‡çš„æ˜¯ç‹¬ç«‹äºç¡¬ä»¶çš„æ—¶é—´
>
> â€‹	- æˆ‘ä»¬æƒ³çŸ¥é“è¿è¡Œæ—¶é—´æ˜¯å¦‚ä½•éšè¾“å…¥çš„å¤§å°è€Œå˜åŒ–çš„
>
> ç©ºé—´ï¼šç®—æ³•æ‰€éœ€çš„ç©ºé—´æœ‰å¤šå¤§

### 1.4 Analysis of insertion sort

Notation: $A[i,j]$ is the subarray of $A$ that starts at position $i$ and ends at position $j$

- **Correctness**: follows from the key observation that after loop $i$, the subarray $A[1,i]$ is sorted.
- **Running time**: number of primitive computational steps
- **Space**: in place algorithm (at most a constant number of elements of A are stored outside A at any time)

#### 1.4.1 Correctness of insertion-sort

Notation: $A[i,j]$ is the subarray of A that starts at position $i$ and ends at position $j$

Minor change in the pseudo code: in line 1, start from $i=1$ rather than $i=2$.

**Claim 1.**

Let $n\ge1$ be a positive integer. For all $1\le i \le n$, after the i-th loop, the subarray $A[1,i]$ is sorted.

Correctness of insertion-sort follows if we show Claim 1.

> Proof of claim 1

By induction on i

- Base case: $i=1$, trivial
- Induction hypothesis: assume that the statement is true for some $1\le i \le n$
- Inductive step: show it true for $i+1$
  - In loop $i+1$, element key = $A[i+1]$ is inserted into $A[1,i]$. By the induction hypothesis, $A[1,i]$ is sorted. Since
    1. key is inserted after the last element $A[l]$ such that $1\le l \le i$ and $A[l]\le key$
    2. all elements in $A[l+1,j]$ are pushed one position to the right with their order preserved

The statement is true for i+1

> æ’å…¥æ’åºçš„æ­£ç¡®æ€§
>
> åŸºæœ¬æƒ…å†µï¼ši=1
>
> è¯±å¯¼å‡è®¾ï¼šå‡è®¾å½“$1\le i \le n$ï¼Œæ’å…¥æ’åºæ˜¯æ­£ç¡®çš„
>
> è¯±å¯¼æ­¥éª¤ï¼š
>
> - åœ¨å¾ªç¯i+1ä¸­ï¼Œå…ƒç´ key=A[i+1]è¢«æ’å…¥åˆ°A[1,i]ã€‚æ ¹æ®è¯±å¯¼å‡è®¾ï¼ŒA[1,n]æ˜¯æœ‰åºçš„ï¼Œå› ä¸ºï¼š
>   - keyæ˜¯è¢«æ’å…¥æœ€åä¸€ä¸ªå°äºç­‰äºkeyçš„å…ƒç´ åé¢
>   - æ‰€æœ‰åœ¨æ­¤è¢«æ’å…¥çš„å…ƒç´ åé¢çš„å…ƒç´ éƒ½è¢«å‘å³ç§»åŠ¨äº†ä¸€ä¸ªå•ä½ï¼Œä»–ä»¬çš„é¡ºåºä¿æŒä¸å˜
> - æ•…è¯¥é™ˆè¿°æ˜¯æ­£ç¡®çš„

![image-20230209151537764](https://images.wu.engineer/images/2023/02/09/image-20230209151537764.png)

#### 1.4.2 Running time $T(n)$ of insertion-sort

```pseudocode
insertion-sort(A)
	for i=2 to n do
		key = A[i]
		// Insert A[i] into the sorted subarray A[1,i-1]
		j = i - 1
		while j>0 and A[j]>key do
			A[j+1] = A[j]
			j = j - 1
		end while
		A[j+1] = key
	end for
```

![image-20230209151817078](https://images.wu.engineer/images/2023/02/09/image-20230209151817078.png)

- For $2\le i \le n$, let $t_i=$ number of times that line 4 is executed. Then

$T(n)=n+3(n-1)+\sum_{i=2}^n t_i+2\sum_{i=2}^n (t_i-1)=3\sum_{i=2}^n t_i+2n-1$

> ç¬¬ä¸€é¡¹$n$ä»£è¡¨æ‰§è¡Œline 1ï¼Œå³forå¾ªç¯çš„æ¬¡æ•°ã€‚ç”±äºä»2åˆ°nä¸€å…±éœ€è¦å¾ªç¯n-1æ¬¡ï¼Œforå¾ªç¯é¢å¤–éœ€è¦ä¸€æ¬¡åˆ¤æ–­ä»¥é€€å‡ºå¾ªç¯ï¼Œæ‰€ä»¥å…±næ¬¡ã€‚
>
> ç¬¬äºŒé¡¹$3(n-1)$ä»£è¡¨æ‰§è¡Œline 2,3,7çš„æ¬¡æ•°ã€‚
>
> ç¬¬ä¸‰é¡¹$\sum_{i=2}^n t_i$ä»£è¡¨æ‰§è¡Œline 4çš„æ¬¡æ•°ï¼Œå…¶ä¸­$t_i$ä»£è¡¨åœ¨æ¯æ¬¡loopä¸­line 4æ‰§è¡Œçš„æ¬¡æ•°ã€‚
>
> ç¬¬å››é¡¹$2\sum_{i=2}^n (t_i-1)$ä»£è¡¨æ‰§è¡Œline 5,6çš„æ¬¡æ•°ã€‚

- Best-case running time:
  - In each loop $i$, the line 4 only execute once. å³line 5,6ä¸è¿è¡Œ
  - So the total number of time that line 4 is executed is $n-1$
  - $3(n-1)+2n-1=5n-4$

- Worst-case running time:
  - $T(n)=\frac {3n^2} {2} +\frac {7n} 2-4$

#### 1.4.3 Worst-case analysis

**Worst-case running time**: largest possible running time of the algorithm over all inputs of a given size n

Why worst-case analysis?

- It gives well-defined computable bounds
- Average-case analysis can be tricky: how do we generate a â€œrandomâ€ instance?

### 1.5 Efficiency of algorithms

#### 1.5.1 Efficiency of insertion-sort and the brute force solution

Compare to brute force solution (è›®åŠ›è§£):

- At each step, generate a new permutation of the $n$ integers
- If sorted, stop and output the permutation

Worst-case analysis: generate $n!$ permutations. Is brute force solution efficient?

- Efficiency relates to the performance of the algorithm as $n$ grows.
- Stirlingâ€™s approximation formula: $n!\approx (\frac ne)^n$
  - For $n=10$, generate $3.67^{10}\ge 2^{10}$ permutations.
  - For $n=100$, generate $36.7^{100}\ge2^{700}$ permutations.

Brute force solution is not efficient.

#### 1.5.2 Attempt 1

**Definition 3**

An algorithm is efficient if it achieves better worst-case performance than brute-force search.

> Caveat: fails to discuss the scaling properties of the algorithm; if the input size grows by a constant factor, we would like the running time T(n) of the algorithm to increase by a constant factor as well.

Polynomial running times: on input of size n, T(n) is at most $c\times n^d$ for c, d > 0 constants

- polynomial running times scale well
- the smaller the exponent of the polynomial the better

**Definition 4**

An algorithm is efficient if it has a polynomial running time.

Caveat

- What about huge constants in front of the leading term or large exponents?

However

- Small degree polynomial running time exist for most problems that can be solved in polynomial time
- Conversely, problems for which no polynomial-time algorithm is know tend to be very hard in practice
- So we can distinguish between easy and hard problems

### 1.6 Running time in terms of # primitive steps

To discuss this, we need a coarser(æ›´ç²—ç³™çš„) classification of running times of algorithms; exact characterisations:

- are to detailed
- do not reveal similarities between running times in an immediate way as n grows large
- are often meaningless: pseudocode steps will expand by a constant factor that depends on the hardware.

### 1.7 Conclusion

In Chapter 1, we:

- Introduced the problem of sorting.
- Analysed insertion-sort
  - Worst-case running time: $T(n)=\frac {3n^2} {2} +\frac {7n} 2-4$
  - Space: in-place algorithm
- Worst-case running time analysis: a reasonable measure of algorithmic efficiency
- Defined polynomial-time algorithms as â€œefficientâ€
- Argued that detailed characterisations of running times are not convenient for understanding scalability of algorithms

## 2. Asymptotic Notation, Mergesort and Recurrences

### 2.1 Asymptotic Notation

A framework that will allow us to compare the rate of growth of different running times as the input size n grows.

- We will express the running time as a function of the number of primitive steps; the latter is a function of the input size n.
- To compare functions expressing running times, we will ignore their low-order terms and focus solely on the highest-order term.

> æ¸è¿›ç¬¦å· ï¼ˆAsymptotic Notationï¼‰
>
> ä¸€ä¸ªç®—æ³•åœ¨ç¼–ç¨‹è¯­è¨€çš„ç¿»è¯‘åæˆä¸ºå¯æ‰§è¡Œç¨‹åºã€‚è€Œè®¡ç®—æœºæ‰§è¡Œè¯¥ç®—æ³•çš„ç¨‹åºéœ€è¦ä¸€å®šçš„æ—¶é—´ï¼Œè¯¥æ—¶é—´çš„é•¿çŸ­å—å¾ˆå¤šæ–¹é¢çš„å½±å“ã€‚å› æ­¤æˆ‘ä»¬ä¸èƒ½ä»…ä»…ä¾é åˆ¤æ–­ç®—æ³•çš„ç¨‹åºæ‰§è¡Œæ—¶é—´æ¥åˆ¤æ–­ç®—æ³•çš„å¥½åã€‚
>
> **è®¡ç®—æœºçš„ç¡¬ä»¶å½±å“æ˜¯â€œæ­»çš„â€ï¼Œè€Œæ•°æ®é‡çš„å½±å“æ˜¯â€œæ´»çš„â€ã€‚**å³è®¡ç®—æœºæ€§èƒ½å·®å¼‚è¿œä¸å¦‚æ•°æ®é‡çš„å¤šå°‘æ‰€å¸¦æ¥çš„å¯¹ç¨‹åºè¿è¡Œæ—¶é—´çš„å½±å“å¤§ã€‚**å¦‚æœå¾…å¤„ç†çš„æ•°æ®é‡å¾ˆå°ï¼Œåˆ™æ— æ³•åŒºåˆ†ç®—æ³•çš„å¥½åï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°†å¾…å¤„ç†çš„æ•°æ®é‡å‡è®¾ä¸ºç‰¹åˆ«å¤§ã€‚**
>
> å½“è¾“å…¥è§„æ¨¡$n$è¶³å¤Ÿå¤§æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å¿½ç•¥ç¡¬ä»¶å·®å¼‚å¸¦æ¥çš„å½±å“ï¼Œåªéœ€è¦å…³æ³¨å½“è¾“å…¥è§„æ¨¡æ— é™å¢åŠ æ—¶ï¼Œ**ç®—æ³•çš„è¿è¡Œæ—¶é—´æ˜¯å¦‚ä½•éšç€è¾“å…¥è§„æ¨¡çš„å˜å¤§è€Œå¢åŠ ã€‚**
>
> å› æ­¤å®šä¹‰äº†$\Omicron,\Omega,\Theta$ç­‰æ¸è¿›ç¬¦å·ï¼Œä¸‹å›¾æ˜¯å¯¹ä»–ä»¬çš„å›¾åƒæè¿°ã€‚
>
> ![image-20230209194208888](https://images.wu.engineer/images/2023/02/09/image-20230209194208888.png)

#### 2.1.1 Asymptotic upper bounds: Big-$\Omicron$ notation

We say that $T(n)=\Omicron(f(n))$ if there exist constants $c>0$ and $n_0\ge0$ s.t. for all $n\ge n_0$, we have $T(n)\le c\times f(n)$

![image-20230209193230521](https://images.wu.engineer/images/2023/02/09/image-20230209193230521.png)

> å¤§$\Omicron$è®°å·
>
> $\Omicron$è®°å·ç»™å‡ºå‡½æ•°çš„æ¸è¿‘ä¸Šç•Œï¼Œå³å½“å‡½æ•°åœ¨å¢é•¿åˆ°ä¸€å®šç¨‹åº¦æ—¶æ€»å°äºç­‰äºä¸€ä¸ªç‰¹å®šå‡½æ•°çš„å¸¸æ•°å€ï¼Œå³$T(n)\le c\times f(n)$ã€‚
>
> ç›¸å½“äºâ€œ$\le$â€

#### 2.1.2 Asymptotic lower bounds: Big-$\Omega$ notation

We say that $T(n)=\Omega(f(n))$ if there exist constants $c>0$ and $n_0\ge0$ s.t. for all $n\ge n_0$, we have $T(n)\ge c\times f(n)$

![image-20230209195339155](https://images.wu.engineer/images/2023/02/09/image-20230209195339155.png)

> å¤§$\Omega$è®°å·
>
> $\Omega$è®°å·ç»™å‡ºå‡½æ•°çš„æ¸è¿‘ä¸‹ç•Œï¼Œå³å½“å‡½æ•°åœ¨å¢é•¿åˆ°ä¸€å®šç¨‹åº¦æ—¶æ€»å¤§äºç­‰äºä¸€ä¸ªç‰¹å®šå‡½æ•°çš„å¸¸æ•°å€ï¼Œå³$T(n)\ge c\times f(n)$ã€‚
>
> ç›¸å½“äºâ€œ$\ge$â€

#### 2.1.3 Asymptotic tight bounds: Big-$\Theta$ notation

We say that $T(n)=\Theta(f(n))$ if there exist constants $c>0$ and $n_0\ge0$ s.t. for all $n\ge n_0$, we have $c_1\times f(n)\le T(n)\le c_2\times f(n)$

![image-20230209195600176](https://images.wu.engineer/images/2023/02/09/image-20230209195600176.png)

> å¤§$\Theta$è®°å·
>
> $\Theta$è®°å·ç»™å‡ºå‡½æ•°çš„æ¸è¿‘ç´§ç¡®ç•Œï¼Œå³$c_1\times f(n)\le T(n)\le c_2\times f(n)$ã€‚
>
> ç›¸å½“äºâ€œ=â€

**Equivalent definition**

$T(n)=\Theta(f(n))$ if $T(n)=\Omicron(f(n))$ and $T(n)=\Omega(f(n))$

#### 2.1.4 Asymptotic upper bounds that are not tight: little $\omicron$

We say that $T(n)=\omicron(f(n))$ if, **for any** constant $c>0$, there exists a constant $n_0\ge0$ such that for all $n\ge n_0$, we have $T(n)<c\times f(n)$

- Intuitively, T(n) becomes **insignificant** relative to f(n) as $n\to \infin$
- Proof by showing that $lim_{n\to\infin}\frac {T(n)}{f(n)}=0$ (if the limit exists)

> å°$\omicron$è®°å·
>
> $\omicron$è®°å·ç»™å‡ºå‡½æ•°çš„éç´§ä¸Šç•Œï¼Œå³å½“å‡½æ•°åœ¨å¢é•¿åˆ°ä¸€å®šç¨‹åº¦æ—¶æ€»å°äºä¸€ä¸ªç‰¹å®šå‡½æ•°çš„å¸¸æ•°å€ï¼Œå³$T(n)\lt c\times f(n)$ã€‚
>
> ç›¸å½“äºâ€œ$\lt$â€

#### 2.1.5 Asymptotic lower bounds that are not tight: little $\omega$

We say that $T(n)=\omega(f(n))$ if, **for any** constant $c>0$, there exists a constant $n_0\ge0$ such that for all $n\ge n_0$, we have $T(n)>c\times f(n)$

- Intuitively, T(n) becomes **arbitrary large** relative to f(n) as $n\to \infin$
- $T(n)=\omega(f(n))$ implies that $lim_{n\to\infin}\frac{T(n)}{f(n)}=\infin$, if the limit exists. Then $f(n)=\omicron(T(n))$

#### 2.1.6 Relationship between asymptotic notations

![image-20230209201043305](https://images.wu.engineer/images/2023/02/09/image-20230209201043305.png)

#### 2.1.7 Basic rules for omitting low order terms from functions

1. Ignore multiplicative factors: e.g., $10n^3$ becomes $n^3$
2. $n^a$ dominates $n^b$ if $a>b$: e.g., $n^2$ dominates n
3. Exponentials dominate polynomials: e.g., $2^n$ dominates $n^4$
4. Polynomials dominate logarithms: e.g., $n$ dominates $log_3n$

For large enough $n$,

$log(n)<n<n\times log(n)<n^2<2^n<3^n<n^n$

#### 2.1.8 Properties of asymptotic growth rates

![image-20230209201505500](https://images.wu.engineer/images/2023/02/09/image-20230209201505500.png)

### 2.2 The Divide & Conquer Principle

The divide & conquer principle åˆ†è£‚ä¸å¾æœåŸåˆ™

- **Divide** the problem into a number of subproblems that are smaller instances of the same problem.
  - Divide the input array into two lists of equal size.
- **Conquer** the subproblems by solving them recursively.
  - Sort each list recursively. (Stop when lists have size 2.)
- **Combine** the solutions to the subproblems to get the solution to the overall problem.
  - Merge the two sorted lists and output the sorted array.

### 2.3 Merge sort

> å½’å¹¶æ’åºï¼ˆMerge sortï¼‰æ˜¯å»ºç«‹åœ¨å½’å¹¶æ“ä½œä¸Šçš„ä¸€ç§æœ‰æ•ˆã€ç¨³å®šçš„æ’åºç®—æ³•ï¼Œè¯¥ç®—æ³•æ˜¯é‡‡ç”¨åˆ†æ²»æ³•(Divide and Conquerï¼‰çš„ä¸€ä¸ªéå¸¸å…¸å‹çš„åº”ç”¨ã€‚å°†å·²æœ‰åºçš„å­åºåˆ—åˆå¹¶ï¼Œå¾—åˆ°å®Œå…¨æœ‰åºçš„åºåˆ—ï¼›å³å…ˆä½¿æ¯ä¸ªå­åºåˆ—æœ‰åºï¼Œå†ä½¿å­åºåˆ—æ®µé—´æœ‰åºã€‚è‹¥å°†ä¸¤ä¸ªæœ‰åºè¡¨åˆå¹¶æˆä¸€ä¸ªæœ‰åºè¡¨ï¼Œç§°ä¸ºäºŒè·¯å½’å¹¶ã€‚
>
> å½“æœ‰ n ä¸ªè®°å½•æ—¶ï¼Œéœ€è¿›è¡Œ logn è½®å½’å¹¶æ’åºï¼Œæ¯ä¸€è½®å½’å¹¶ï¼Œå…¶æ¯”è¾ƒæ¬¡æ•°ä¸è¶…è¿‡ nï¼Œå…ƒç´ ç§»åŠ¨æ¬¡æ•°éƒ½æ˜¯ nï¼Œå› æ­¤ï¼Œå½’å¹¶æ’åºçš„æ—¶é—´å¤æ‚åº¦ä¸º O(nlogn)ã€‚å½’å¹¶æ’åºæ—¶éœ€è¦å’Œå¾…æ’åºè®°å½•ä¸ªæ•°ç›¸ç­‰çš„å­˜å‚¨ç©ºé—´ï¼Œæ‰€ä»¥ç©ºé—´å¤æ‚åº¦ä¸º O(n)ã€‚
>
> å½’å¹¶æ’åºé€‚ç”¨äºæ•°æ®é‡å¤§ï¼Œå¹¶ä¸”å¯¹ç¨³å®šæ€§æœ‰è¦æ±‚çš„åœºæ™¯ã€‚
>
> **è¿‡ç¨‹ï¼š**
>
> å½’å¹¶æ’åºæ˜¯é€’å½’ç®—æ³•çš„ä¸€ä¸ªå®ä¾‹ï¼Œè¿™ä¸ªç®—æ³•ä¸­åŸºæœ¬çš„æ“ä½œæ˜¯åˆå¹¶ä¸¤ä¸ªå·²æ’åºçš„æ•°ç»„ï¼Œå–ä¸¤ä¸ªè¾“å…¥æ•°ç»„ A å’Œ Bï¼Œä¸€ä¸ªè¾“å‡ºæ•°ç»„ Cï¼Œä»¥åŠä¸‰ä¸ªè®¡æ•°å™¨ iã€jã€kï¼Œå®ƒä»¬åˆå§‹ä½ç½®ç½®äºå¯¹åº”æ•°ç»„çš„å¼€å§‹ç«¯ã€‚
>
> A[i] å’Œ B[j] ä¸­è¾ƒå°è€…æ‹·è´åˆ° C ä¸­çš„ä¸‹ä¸€ä¸ªä½ç½®ï¼Œç›¸å…³è®¡æ•°å™¨å‘å‰æ¨è¿›ä¸€æ­¥ã€‚
>
> å½“ä¸¤ä¸ªè¾“å…¥æ•°ç»„æœ‰ä¸€ä¸ªç”¨å®Œæ—¶å€™ï¼Œåˆ™å°†å¦å¤–ä¸€ä¸ªæ•°ç»„ä¸­å‰©ä½™éƒ¨åˆ†æ‹·è´åˆ° C ä¸­ã€‚
>
> ![image-20230209202103727](https://images.wu.engineer/images/2023/02/09/image-20230209202103727.png)
>
> ![File:Merge-sort-example-300px.gif](https://upload.wikimedia.org/wikipedia/commons/c/cc/Merge-sort-example-300px.gif?20151222172210)

**Remarks**:

- Merge sort is a recursive procedure
- Initial call: mergesort(A,1,n)
- Subroutine merge merges two sorted lists of size $\lfloor n/2 \rfloor,\ \lceil n/2 \rceil$ into one sorted list of size n

**Intuition: **to merge two sorted lists of size n/2 repeatedly

- compare the two items in the front of the two lists
- extract the smaller item and append it to the output
- update the front of the list from which the item was extracted

#### 2.3.1 Codes

**Pseudo Code**

```pseudocode
merge(A, left, right, mid)
	L = A[left, mid]
	R = A[mid+1, right]
	Maintain two pointers p_l, p_r, initialised to point to the first elements of L,R, repectively
	while both lists are non-empty do
		let x,y be the elements pointed to by p_l, p_r
		compare x,y and append the smaller to the output
		advance the pointer in the list with the smaller of x,y
	end while
	append the remainder of the non-empty list to the output
mergesort(A, left, right)
	if right == left then return
	end if
	mid = left + lower_bound((right-left)/2) //ä¸‹å–æ•´(right-left)/2
	mergesort(A, left, mid)
	mergesort(A, mid+1, right)
	merge(A, left, right, mid)
```

**C++**

```c++
// C++ program for Merge Sort
#include <iostream>
using namespace std;

// Merges two subarrays of array[].
// First subarray is arr[begin..mid]
// Second subarray is arr[mid+1..end]
void merge(int array[], int const left, int const mid,
		int const right)
{
	auto const subArrayOne = mid - left + 1;
	auto const subArrayTwo = right - mid;

	// Create temp arrays
	auto *leftArray = new int[subArrayOne],
		*rightArray = new int[subArrayTwo];

	// Copy data to temp arrays leftArray[] and rightArray[]
	for (auto i = 0; i < subArrayOne; i++)
		leftArray[i] = array[left + i];
	for (auto j = 0; j < subArrayTwo; j++)
		rightArray[j] = array[mid + 1 + j];

	auto indexOfSubArrayOne
		= 0, // Initial index of first sub-array
		indexOfSubArrayTwo
		= 0; // Initial index of second sub-array
	int indexOfMergedArray
		= left; // Initial index of merged array

	// Merge the temp arrays back into array[left..right]
	while (indexOfSubArrayOne < subArrayOne
		&& indexOfSubArrayTwo < subArrayTwo) {
		if (leftArray[indexOfSubArrayOne]
			<= rightArray[indexOfSubArrayTwo]) {
			array[indexOfMergedArray]
				= leftArray[indexOfSubArrayOne];
			indexOfSubArrayOne++;
		}
		else {
			array[indexOfMergedArray]
				= rightArray[indexOfSubArrayTwo];
			indexOfSubArrayTwo++;
		}
		indexOfMergedArray++;
	}
	// Copy the remaining elements of
	// left[], if there are any
	while (indexOfSubArrayOne < subArrayOne) {
		array[indexOfMergedArray]
			= leftArray[indexOfSubArrayOne];
		indexOfSubArrayOne++;
		indexOfMergedArray++;
	}
	// Copy the remaining elements of
	// right[], if there are any
	while (indexOfSubArrayTwo < subArrayTwo) {
		array[indexOfMergedArray]
			= rightArray[indexOfSubArrayTwo];
		indexOfSubArrayTwo++;
		indexOfMergedArray++;
	}
	delete[] leftArray;
	delete[] rightArray;
}

// begin is for left index and end is
// right index of the sub-array
// of arr to be sorted */
void mergeSort(int array[], int const begin, int const end)
{
	if (begin >= end)
		return; // Returns recursively

	auto mid = begin + (end - begin) / 2;
	mergeSort(array, begin, mid);
	mergeSort(array, mid + 1, end);
	merge(array, begin, mid, end);
}

// UTILITY FUNCTIONS
// Function to print an array
void printArray(int A[], int size)
{
	for (auto i = 0; i < size; i++)
		cout << A[i] << " ";
}

// Driver code
int main()
{
	int arr[] = { 12, 11, 13, 5, 6, 7 };
	auto arr_size = sizeof(arr) / sizeof(arr[0]);

	cout << "Given array is \n";
	printArray(arr, arr_size);

	mergeSort(arr, 0, arr_size - 1);

	cout << "\nSorted array is \n";
	printArray(arr, arr_size);
	return 0;
}
```

**Python**

```python

# Python program for implementation of MergeSort
def mergeSort(arr):
    if len(arr) > 1:
 
         # Finding the mid of the array
        mid = len(arr)//2
 
        # Dividing the array elements
        L = arr[:mid]
 
        # into 2 halves
        R = arr[mid:]
 
        # Sorting the first half
        mergeSort(L)
 
        # Sorting the second half
        mergeSort(R)
 
        i = j = k = 0
 
        # Copy data to temp arrays L[] and R[]
        while i < len(L) and j < len(R):
            if L[i] <= R[j]:
                arr[k] = L[i]
                i += 1
            else:
                arr[k] = R[j]
                j += 1
            k += 1
 
        # Checking if any element was left
        while i < len(L):
            arr[k] = L[i]
            i += 1
            k += 1
 
        while j < len(R):
            arr[k] = R[j]
            j += 1
            k += 1
 
# Code to print the list
 
 
def printList(arr):
    for i in range(len(arr)):
        print(arr[i], end=" ")
    print()
 
 
# Driver Code
if __name__ == '__main__':
    arr = [12, 11, 13, 5, 6, 7]
    print("Given array is", end="\n")
    printList(arr)
    mergeSort(arr)
    print("Sorted array is: ", end="\n")
    printList(arr)
```

**Java**

```java
/* Java program for Merge Sort */
class MergeSort {
    // Merges two subarrays of arr[].
    // First subarray is arr[l..m]
    // Second subarray is arr[m+1..r]
    void merge(int arr[], int l, int m, int r)
    {
        // Find sizes of two subarrays to be merged
        int n1 = m - l + 1;
        int n2 = r - m;
 
        /* Create temp arrays */
        int L[] = new int[n1];
        int R[] = new int[n2];
 
        /*Copy data to temp arrays*/
        for (int i = 0; i < n1; ++i)
            L[i] = arr[l + i];
        for (int j = 0; j < n2; ++j)
            R[j] = arr[m + 1 + j];
 
        /* Merge the temp arrays */
 
        // Initial indexes of first and second subarrays
        int i = 0, j = 0;
 
        // Initial index of merged subarray array
        int k = l;
        while (i < n1 && j < n2) {
            if (L[i] <= R[j]) {
                arr[k] = L[i];
                i++;
            }
            else {
                arr[k] = R[j];
                j++;
            }
            k++;
        }
 
        /* Copy remaining elements of L[] if any */
        while (i < n1) {
            arr[k] = L[i];
            i++;
            k++;
        }
 
        /* Copy remaining elements of R[] if any */
        while (j < n2) {
            arr[k] = R[j];
            j++;
            k++;
        }
    }
 
    // Main function that sorts arr[l..r] using
    // merge()
    void sort(int arr[], int l, int r)
    {
        if (l < r) {
            // Find the middle point
            int m = l + (r - l) / 2;
 
            // Sort first and second halves
            sort(arr, l, m);
            sort(arr, m + 1, r);
 
            // Merge the sorted halves
            merge(arr, l, m, r);
        }
    }
 
    /* A utility function to print array of size n */
    static void printArray(int arr[])
    {
        int n = arr.length;
        for (int i = 0; i < n; ++i)
            System.out.print(arr[i] + " ");
        System.out.println();
    }
 
    // Driver code
    public static void main(String args[])
    {
        int arr[] = { 12, 11, 13, 5, 6, 7 };
 
        System.out.println("Given Array");
        printArray(arr);
 
        MergeSort ob = new MergeSort();
        ob.sort(arr, 0, arr.length - 1);
 
        System.out.println("\nSorted array");
        printArray(arr);
    }
}
```

#### 2.3.2 Analysis of mergesort

1. Correctness: by induction on the size of the two lists

   For simplicity, assume $n=2^k$ for integer $k\ge0$

   We will use induction on $k$.

   - Base case: For $k=0$, the input consists of 1 item; mergesort return the item

   - Induction hypothesis: For $k\ge0$, assume that mergesort correctly sorts any list of size $2^k$

   - Induction step: We will show that mergesort correctly sorts any list A of size $2^{k+1}$

     From the pseudocode of mergesort, we have:

     - Line 3: mid takes the value $2^k$
     - Line 4: mergesort(A,1,$2^k$) correctly sorts the leftmost half of the input, by the induction hypothesis
     - Line 5: mergesort(A,$2^k$+1,$2^{k+1}$) correctly sorts the rightmost half of the input, by the induction hypothesis
     - Line 6: merge correctly merges its two sorted input lists into one sorted output of size $2^k+2^k$

     -> mergesort correctly sorts any input of size $2^{k+1}$

1. Running time

   ![image-20230209203930005](https://images.wu.engineer/images/2023/02/09/image-20230209203930005.png)

   - Suppose L, R have n/2 elements each
   - How many iterations before all elements from both lists have been appended to the output? At most n-1.
   - How much work within each iteration? constant.

   -> merge takes $\Omicron(n)$ time to merge L, R

2. Space

   - Extra $\Theta(n)$ space to store L, R (the output of merge is stored directly in A)

### 2.4 Solving recurrences and running time of mergesort

è§£å†³é€’å½’é—®é¢˜å’Œåˆå¹¶æ’åºçš„è¿è¡Œæ—¶é—´

#### 2.4.1 Solving recurrences, method 1: recursion trees

The recursion trees (é€’å½’æ ‘) consists of three steps:

1. Analyse the first few levels of the tree of recursive calls
2. Identify a pattern
3. Sum the work spent over all levels of recursion

#### 2.4.2 A general recurrence and its solution

The running time of many recursive algorithms can be expressed by the following recurrence
$$
T(n)=aT(n/b)+cn^k,\ \text{for a, c>0, b>1, }k\ge0
$$
What is the recursion tree for this recurrence?

- a is the branching factor
- b is the factor by which the size of each subprobelm shrinks

-> at level i, there are $a^i$ subproblems, each of size $n/b^i$

-> each subproblem at level i requires $c(n/b^i)^k$ work

- The height of the tree is $log_bn$ levels

-> Total work: $\sum_{i=0}^{log_bn}a^ic(n/b^i)^k=cn^k\sum_{i=0}^{log_bn}(\frac a{b^k})^i$

#### 2.4.3 Solving recurrences, method 2: Master theorm

> Theorem 6 (Master Theorem)

If $T(n)=aT(\lceil n/b \rceil)+\Omicron(n^k)$ for some constants $a>0,b>1,k\ge0$, then
$$
T(n) =
\begin{cases}
\Omicron(n^{log_ba}), & if \ a>b^k \\
\Omicron(n^klogn), & if\ a=b^k\\
\Omicron(n^k), & if\ a<b^k
\end{cases}
$$

### 2.4.4 Solving recurrences, method 3: the substitution method

The technique consists of two steps

1. Guess a bound
2. Use (strong) induction to prove that the guess is correct

> 1. Simple induction: the induction step at $n$ requires that the inductive hypothesis holds at step $n-1$
> 2. Strong induction: is just a variant of simple induction where the induction step at $n$ requires that inductive hypothesis holds at **all previous steps** $1,2,...,n-1$

### 2.5 Conclusion

In Chapter 2, we discussed:

- **Asymptotic notation** ($\Omicron, \Omega, \Theta, \omicron, \omega$)
- **The divide & conquer principle**
  - **Divide** the problem into a number of subproblems that are smaller instances of the same problem.
    - Divide the input array into two lists of equal size.
  - **Conquer** the subproblems by solving them recursively.
    - Sort each list recursively. (Stop when lists have size 2.)
  - **Combine** the solutions to the subproblems to get the solution to the overall problem.
    - Merge the two sorted lists and output the sorted array.

- **Application: mergesort**

  - ```pseudocode
    merge(A, left, right, mid)
    	L = A[left, mid]
    	R = A[mid+1, right]
    	Maintain two pointers p_l, p_r, initialised to point to the first elements of L,R, repectively
    	while both lists are non-empty do
    		let x,y be the elements pointed to by p_l, p_r
    		compare x,y and append the smaller to the output
    		advance the pointer in the list with the smaller of x,y
    	end while
    	append the remainder of the non-empty list to the output
    mergesort(A, left, right)
    	if right == left then return
    	end if
    	mid = left + lower_bound((right-left)/2) //ä¸‹å–æ•´(right-left)/2
    	mergesort(A, left, mid)
    	mergesort(A, mid+1, right)
    	merge(A, left, right, mid)
    ```

- **Solving recurrences**
  - Recursion trees
  - Master theorem
  - Substitution method

## 3. Divide & conquer algorithms: fast int/matrix multiplication

### 3.1 Binary search

- Input:
  1. sorted list A of n integers
  2. integer x
- Output:
  - index j such that $1\le j \le n$ and $A[j]=x$, or
  - no if x is not in $A$

*Example: $A={0,2,3,5,6,7,9,11,13},\ n=9,\ x=7$*

Idea: use the fact that the array is **sorted** and probe specific entries in the array



> ##### From EEEN30002 Numerical Analysis
>
> ğŸ’¡ å°†å¤æ‚çš„éçº¿æ€§æ–¹ç¨‹é—®é¢˜ç®€åŒ–ï¼Œå°†éçº¿æ€§æ–¹ç¨‹åˆ†æˆå‡ ä¸ªåŒºé—´ï¼Œå¯¹æ¯ä¸ªåŒºé—´åˆ†åˆ«æ±‚è§£ï¼Œé€‰å–ä¸€ä¸ªè¿‘ä¼¼åŒºé—´ä½¿ç”¨è¿­ä»£æ³•é€¼è¿‘çœŸå®è§£
>
> ### æ­¥éª¤1
>
> åœ¨å‡½æ•°ä¸Šé€‰å–ä¸¤ä¸ªç‚¹aï¼Œbï¼Œ**ç¡®ä¿f(a)\*f(b)<0**ã€‚å³abä¸¤ç‚¹åœ¨å‡½æ•°å›¾åƒä¸Šçš„åˆ†å¸ƒä¸ºï¼š**ä¸€ä¸ªåœ¨å‡½æ•°æ ¹çš„å·¦ä¾§ï¼Œä¸€ä¸ªåœ¨å‡½æ•°æ ¹çš„å³ä¾§**ã€‚
>
> ### æ­¥éª¤2
>
> è®¾è¿‘ä¼¼è·Ÿ$x_m = (a + b)/2$
>
> ![image-20230210171107592](https://images.wu.engineer/images/2023/02/10/image-2023021017110759227cf0b656ac70b78.png)
>
> ![image-20230210171122872](https://images.wu.engineer/images/2023/02/10/image-20230210171122872.png)
>
> ### æ­¥éª¤3
>
> åˆ¤æ–­ï¼š
>
> - å¦‚æœ$f(a)*f(x_m)<0$ï¼Œåˆ™å‡½æ•°çš„çœŸå®è§£åœ¨aå’Œmä¹‹é—´
>   - $a = a; b = x_m$
> - å¦‚æœ$f(a)*f(x_m)>0$ï¼Œåˆ™å‡½æ•°çš„çœŸå®è§£åœ¨må’Œbä¹‹é—´
>   - $a = x_m; b = b$
> - å¦‚æœ$f(a)*f(x_m)=0$ï¼Œåˆ™å‡½æ•°çš„çœŸå®è§£ä¸ºaï¼Œåœæ­¢ç®—æ³•
>
> ### æ­¥éª¤4
>
> å¯»æ‰¾æ–°çš„è¿‘ä¼¼æ ¹$x_m=(a+b)/2$
>
> è®¡ç®—ç»å¯¹ç›¸å¯¹ä¼°è®¡è¯¯å·®$e_k<=1/2(a_{prev}-b_{prev})=(1/2)^{k+1}*(a_0-b_o)$
>
> ğŸ’¡ $a_{prev},b_{prev}ä¸ºaå’Œbçš„ä¸Šæ¬¡çš„å€¼ï¼Œa_0,b_0ä¸ºaå’Œbç¬¬ä¸€æ¬¡ä¼°è®¡çš„å€¼$
>
> > ç»å¯¹è¯¯å·®$e_k$åº”è¯¥å°äºä¸Šä¸€æ¬¡è®¡ç®—å‡ºçš„è¯¯å·®ï¼ŒåŒæ—¶åº”è¯¥ç­‰äº$(1/2)^{k+1}*(a_0-b_o)$ï¼Œå› ä¸ºäºŒåˆ†ï¼Œæ‰€ä»¥è¯¯å·®å€¼å¯ä»¥è®¡ç®—
>
> ### æ­¥éª¤5
>
> å°†ç»å¯¹ç›¸å¯¹è¯¯å·®$e_k$å’Œäº‹å…ˆè®¾å®šçš„epsilonå€¼åšæ¯”è¾ƒï¼Œå¦‚æœ$e_k<epsilon$ï¼Œåˆ™ç®—æ³•åœæ­¢ï¼Œå¦åˆ™ç®—æ³•ç»§ç»­
>
> ### æ ¹æ®åˆå§‹çš„abå’Œepsilonå€¼åˆ¤æ–­éœ€è¦å¤šå°‘ä¸ªå¾ªç¯æ‰èƒ½æ»¡è¶³æ¡ä»¶
>
> å‡è®¾æˆ‘ä»¬éœ€è¦$e_k<epsilon$
>
> $(1/2)^{k+1}*(a_0-b_0)<epsilon$
>
> å¯å¾—$k+1>(ln(a_0-b_0)-ln(epsilon))/(ln(2))$
>
> ### MATLAB
>
> ```matlab
> %%% bisection algorithm to find sqrt(2)
> 
> xmin = 0;
> xmax = 2;
> fmin = xmin^2-2;
> fmax = xmax^2-2;
> 
> %%% choose epsilon
> epsilon = 10^-3/2;
> 
> for k = 0 : ceil((log(2)-log(epsilon))/log(2) -1),
>     
>     xhat = (xmin+xmax)/2;
>     fhat = xhat^2-2;
>         
>     disp([k xmin xmax xhat abs(xhat-sqrt(2)) (xmax-xmin)/2 fmin fmax fhat])
>     
>     if fhat*fmin > 0,
>         xmin = xhat; fmin = fhat;
>     else
>         xmax = xhat; fmax = fhat;
>     end
>     
> end
> ```
>
> ### C++
>
> ```cpp
> #include <iostream>
> using namespace std;
> #define EP 0.01
> // An example function whose solution is determined using
> // Bisection Method. The function is x^3 - x^2 + 2
> double solution(double x) {
>    return x*x*x - x*x + 2;
> }
> // Prints root of solution(x) with error in EPSILON
> void bisection(double a, double b) {
>    if (solution(a) * solution(b) >= 0) {
>       cout << "You have not assumed right a and b\\n";
>       return;
>    }
>    double c = a;
>    while ((b-a) >= EP) {
>       // Find middle point
>       c = (a+b)/2;
>       // Check if middle point is root
>       if (solution(c) == 0.0)
>          break;
>        // Decide the side to repeat the steps
>       else if (solution(c)*solution(a) < 0)
>          b = c;
>       else
>          a = c;
>    }
>    cout << "The value of root is : " << c;
> }
>  // main function
> int main() {
>    double a =-500, b = 100;
>    bisection(a, b);
>    return 0;
> }
> ```